{
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushka108/Transformers-in-NLP/blob/main/Transformers_in_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers in a Nutshell\n",
        "An educational but usable example of a (character level) GPT-2 transformer language model."
      ],
      "metadata": {
        "id": "2LRRETL76Dly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wget pytorch-lightning"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T20:01:41.217367Z",
          "iopub.execute_input": "2024-07-09T20:01:41.217755Z",
          "iopub.status.idle": "2024-07-09T20:01:58.599107Z",
          "shell.execute_reply.started": "2024-07-09T20:01:41.217723Z",
          "shell.execute_reply": "2024-07-09T20:01:58.597757Z"
        },
        "trusted": true,
        "id": "0s_ASSr9jutG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import wget\n",
        "from tqdm import tqdm\n",
        "\n",
        "# for dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# for model\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.functional import accuracy\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "1PXGA-IrOqx9",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:01:58.601505Z",
          "iopub.execute_input": "2024-07-09T20:01:58.601855Z",
          "iopub.status.idle": "2024-07-09T20:02:05.891676Z",
          "shell.execute_reply.started": "2024-07-09T20:01:58.601823Z",
          "shell.execute_reply": "2024-07-09T20:02:05.890728Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "* vocab_size: The size of the vocabulary. This should be set later based on the dataset.\n",
        "* max_seq_len: The maximum sequence length for the input text.\n",
        "* emb_size: The size of the embeddings.\n",
        "* num_blocks: The number of transformer blocks (layers).\n",
        "* num_heads: The number of attention heads in each transformer block.\n",
        "* fc_hidden_dim: The hidden dimension size of the feed-forward network within each transformer block, typically 4 times the embedding size.\n"
      ],
      "metadata": {
        "id": "4Qpz6QHZjutH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Config:\n",
        "    \"\"\"\n",
        "    'gpt2-mini' config from minGPT\n",
        "    \"\"\"\n",
        "    # data\n",
        "    default_data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "    # model\n",
        "    vocab_size = None\n",
        "    max_seq_len = 128\n",
        "    emb_size = 192\n",
        "    num_blocks = 6\n",
        "    num_heads = 6\n",
        "    fc_hidden_dim = 4 * emb_size\n",
        "\n",
        "    # regularization\n",
        "    attn_dropout_p = 0.1  # Dropout probability for the attention layers.\n",
        "    res_dropout_p = 0.1   #Dropout probability for the residual connections.\n",
        "    emb_dropout_p = 0.1   #Dropout probability for the embeddings.\n",
        "\n",
        "    # training\n",
        "    max_learning_rate = 2.5e-4\n",
        "    batch_size = 512\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\" any extra config args \"\"\"\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)"
      ],
      "metadata": {
        "id": "hiJfSevUO2qA",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:05.892816Z",
          "iopub.execute_input": "2024-07-09T20:02:05.893229Z",
          "iopub.status.idle": "2024-07-09T20:02:05.918941Z",
          "shell.execute_reply.started": "2024-07-09T20:02:05.893202Z",
          "shell.execute_reply": "2024-07-09T20:02:05.917957Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, config, data=None):\n",
        "        \"\"\"\n",
        "        A toy dataset class for charGPT modified from the minGPT repo\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        if data is None:\n",
        "            filename = wget.download(config.default_data_url)\n",
        "            data = open(filename, 'r').read()\n",
        "\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }  #stoi: A dictionary mapping each character to a unique integer (character to index).\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }  #itos: A dictionary mapping each integer to its corresponding character (index to character).\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.max_seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.max_seq_len + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "m_pedIh6PDxB",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:05.921972Z",
          "iopub.execute_input": "2024-07-09T20:02:05.922371Z",
          "iopub.status.idle": "2024-07-09T20:02:05.933656Z",
          "shell.execute_reply.started": "2024-07-09T20:02:05.922336Z",
          "shell.execute_reply": "2024-07-09T20:02:05.932422Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Ensure embedding size is divisible by the number of attention heads\n",
        "        assert config.emb_size % config.num_heads == 0\n",
        "\n",
        "        # Linear layers to compute query, key, and value projections\n",
        "        self.W_Q = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "        self.W_K = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "        self.W_V = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "        # Linear layer to project the concatenated output of all heads\n",
        "        self.res_proj = nn.Linear(config.emb_size, config.emb_size, bias=False)\n",
        "\n",
        "        # Dropout layers for regularization\n",
        "        self.attn_dropout = nn.Dropout(config.attn_dropout_p)\n",
        "        self.res_dropout = nn.Dropout(config.res_dropout_p)\n",
        "\n",
        "        # Register a lower triangular matrix to enforce causality in attention\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
        "        )\n",
        "\n",
        "        # Store the number of attention heads\n",
        "        self.num_heads = config.num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # step 0) size: (batch_size, seq_len, emb_size)\n",
        "        batch_size, seq_len, emb_size = x.size()\n",
        "        # Compute the dimension of each attention head\n",
        "        head_dim = emb_size // self.num_heads\n",
        "\n",
        "        # step 1) Project the input `x` to queries, keys, and values\n",
        "        # Reshape and split into multiple heads\n",
        "        # size: (batch_size, seq_len, emb_size) -> (batch_size, seq_len, num_heads, head_dim)\n",
        "        Q = self.W_Q(x).reshape(batch_size, seq_len, self.num_heads, head_dim)\n",
        "        K = self.W_K(x).reshape(batch_size, seq_len, self.num_heads, head_dim)\n",
        "        V = self.W_V(x).reshape(batch_size, seq_len, self.num_heads, head_dim)\n",
        "\n",
        "        # step 2) Transpose to get the heads dimension first\n",
        "        # size: (batch_size, seq_len, num_heads, head_dim) -> (batch_size, num_heads, seq_len, head_dim)\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        # step 3) Compute the attention scores\n",
        "        # size: (batch_size, num_heads, seq_len, head_dim) x (batch_size, num_heads, head_dim, seq_len)\n",
        "        # = (batch_size, num_heads, seq_len, seq_len)\n",
        "        scores = Q @ K.transpose(-2, -1) * (1.0 / math.sqrt(head_dim))\n",
        "\n",
        "        # step 4) Apply the causal mask to the attention scores\n",
        "        # Mask future positions (set to -inf)\n",
        "        scores = scores.masked_fill(self.mask[:seq_len, :seq_len] == 0, float('-inf'))\n",
        "\n",
        "        # step 5) Apply softmax to get the attention weights\n",
        "        # size: (batch_size, num_heads, seq_len, seq_len)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        # step 6) Compute the weighted sum of values\n",
        "        # size: (batch_size, num_heads, seq_len, seq_len) x (batch_size, num_heads, seq_len, head_dim)\n",
        "        # = (batch_size, num_heads, seq_len, head_dim)\n",
        "        out = attn @ V\n",
        "\n",
        "        # step 7) Transpose and reshape to concatenate heads back\n",
        "        # size: (batch_size, num_heads, seq_len, head_dim) -> (batch_size, seq_len, emb_size)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, emb_size)\n",
        "\n",
        "        # step 8) Project concatenated heads back into the embedding space\n",
        "        out = self.res_proj(out)\n",
        "        out = self.res_dropout(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "-jMwDArhowby",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:05.935308Z",
          "iopub.execute_input": "2024-07-09T20:02:05.93563Z",
          "iopub.status.idle": "2024-07-09T20:02:05.952331Z",
          "shell.execute_reply.started": "2024-07-09T20:02:05.935604Z",
          "shell.execute_reply": "2024-07-09T20:02:05.951337Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Linear layer to project input into hidden dimension\n",
        "        self.hidden = nn.Linear(config.emb_size, config.fc_hidden_dim)\n",
        "        # Activation function (Gaussian Error Linear Unit)\n",
        "        self.gelu = nn.GELU()\n",
        "        # Linear layer to project hidden dimension back to embedding size\n",
        "        self.res_proj = nn.Linear(config.fc_hidden_dim, config.emb_size)\n",
        "        # Dropout layer for regularization\n",
        "        self.res_dropout = nn.Dropout(config.res_dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first linear transformation to the input\n",
        "        x = self.hidden(x)\n",
        "        # Apply GELU activation function\n",
        "        x = self.gelu(x)\n",
        "        # Project back to the original embedding size\n",
        "        x = self.res_proj(x)\n",
        "        # Apply dropout for regularization\n",
        "        x = self.res_dropout(x)\n",
        "\n",
        "        # Return the final output\n",
        "        return x"
      ],
      "metadata": {
        "id": "SRLVYVQ-ZHLD",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:05.953625Z",
          "iopub.execute_input": "2024-07-09T20:02:05.953995Z",
          "iopub.status.idle": "2024-07-09T20:02:05.970648Z",
          "shell.execute_reply.started": "2024-07-09T20:02:05.953963Z",
          "shell.execute_reply": "2024-07-09T20:02:05.969797Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Layer normalization for the self-attention mechanism\n",
        "        self.ln1 = nn.LayerNorm(config.emb_size)\n",
        "        # Causal Multi-Head Attention mechanism\n",
        "        self.attn = CausalMultiHeadAttention(config)\n",
        "        # Layer normalization after the attention mechanism\n",
        "        self.ln2 = nn.LayerNorm(config.emb_size)\n",
        "        # Multi-Layer Perceptron (MLP) for the feedforward network\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply layer normalization to the input\n",
        "        x = self.ln1(x)\n",
        "        # Perform self-attention and add the residual connection\n",
        "        x = x + self.attn(x)\n",
        "        # Apply layer normalization to the output of the attention mechanism\n",
        "        x = self.ln2(x)\n",
        "        # Apply the MLP and add the residual connection\n",
        "        x = x + self.mlp(x)\n",
        "\n",
        "        # Return the final output\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZejHjDOLgdtz",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:05.97186Z",
          "iopub.execute_input": "2024-07-09T20:02:05.972184Z",
          "iopub.status.idle": "2024-07-09T20:02:05.983919Z",
          "shell.execute_reply.started": "2024-07-09T20:02:05.972159Z",
          "shell.execute_reply": "2024-07-09T20:02:05.982779Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.emb_size)\n",
        "        # Positional embedding layer\n",
        "        self.pos_emb = nn.Embedding(config.max_seq_len, config.emb_size)\n",
        "        # Dropout layer for embeddings\n",
        "        self.emb_dropout = nn.Dropout(config.emb_dropout_p)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config.num_blocks)])\n",
        "        # Layer normalization after transformer blocks\n",
        "        self.ln = nn.LayerNorm(config.emb_size)\n",
        "        # Final linear layer for prediction\n",
        "        self.head = nn.Linear(config.emb_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Parameter to store positional indices\n",
        "        self.pos_idxs = nn.Parameter(torch.arange(0, config.max_seq_len), requires_grad=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # Token embeddings\n",
        "        tok_embs = self.tok_emb(x)\n",
        "        # Positional embeddings\n",
        "        pos_embs = self.pos_emb(self.pos_idxs[:seq_len])\n",
        "\n",
        "        # Combine token and positional embeddings, apply dropout\n",
        "        seq = self.emb_dropout(tok_embs + pos_embs)\n",
        "\n",
        "        # Transformer blocks\n",
        "        seq = self.blocks(seq)\n",
        "\n",
        "        # Layer normalization\n",
        "        seq = self.ln(seq)\n",
        "\n",
        "        # Final linear layer for prediction\n",
        "        out = self.head(seq)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Initialize weights for linear layers\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            # Initialize weights for embedding layers\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            # Initialize weights for layer normalization\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "        # Special initialization for residual projection weights in TransformerBlocks\n",
        "        for name, param in self.named_parameters():\n",
        "            if name.endswith('res_proj.weight'):\n",
        "                # Initialize with normal distribution scaled by sqrt(1/2 * num_blocks)\n",
        "                torch.nn.init.normal_(param, mean=0.0, std=0.02 / math.sqrt(2 * self.config.num_blocks))"
      ],
      "metadata": {
        "id": "XsJ6vhlMhsCB",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:05.985349Z",
          "iopub.execute_input": "2024-07-09T20:02:05.985738Z",
          "iopub.status.idle": "2024-07-09T20:02:06.001768Z",
          "shell.execute_reply.started": "2024-07-09T20:02:05.985704Z",
          "shell.execute_reply": "2024-07-09T20:02:06.000862Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2LitModel(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model  # GPT-2 model\n",
        "        self.config = config  # Configuration parameters\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)  # Forward pass through the model\n",
        "        # Compute loss using cross-entropy\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "        self.log('train_loss', loss)  # Log training loss\n",
        "        return loss\n",
        "    def configure_optimizers(self):\n",
        "        # Configure Adam optimizer with maximum learning rate from config\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.max_learning_rate)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "WGV1V6gnnNgh",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:06.003106Z",
          "iopub.execute_input": "2024-07-09T20:02:06.003791Z",
          "iopub.status.idle": "2024-07-09T20:02:06.0176Z",
          "shell.execute_reply.started": "2024-07-09T20:02:06.003755Z",
          "shell.execute_reply": "2024-07-09T20:02:06.016729Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wiring everything up to start training\n",
        "config = GPT2Config()\n",
        "dataset = CharDataset(config)\n",
        "config.vocab_size = dataset.vocab_size\n",
        "\n",
        "train_loader = DataLoader(dataset, num_workers=4, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "model = GPT2(config)\n",
        "lit_model = GPT2LitModel(model, config)\n",
        "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=10)"
      ],
      "metadata": {
        "id": "Qqri2b57nXNx",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:06.020718Z",
          "iopub.execute_input": "2024-07-09T20:02:06.021027Z",
          "iopub.status.idle": "2024-07-09T20:02:08.084225Z",
          "shell.execute_reply.started": "2024-07-09T20:02:06.021003Z",
          "shell.execute_reply": "2024-07-09T20:02:08.083352Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(lit_model, train_loader)"
      ],
      "metadata": {
        "id": "vh1wx8RdBkUu",
        "execution": {
          "iopub.status.busy": "2024-07-09T20:02:08.085248Z",
          "iopub.execute_input": "2024-07-09T20:02:08.085531Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save our trained model so we can use it later\n",
        "model_save_name = 'shakespeareGPT.pt'\n",
        "path = f'/kaggle/working/{model_save_name}'\n",
        "torch.save(lit_model.state_dict(), path)"
      ],
      "metadata": {
        "id": "dH6-ZDGl77qg",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple helper function to prompt model and get readable result\n",
        "@torch.no_grad()\n",
        "def get_predictions(model, prompt, max_seq_len=128):\n",
        "    input = torch.LongTensor([dataset.stoi[i] for i in prompt]).unsqueeze(0)\n",
        "    while input.size(1) < max_seq_len:\n",
        "        logits = model(input)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits)\n",
        "        idxs = torch.multinomial(probs, num_samples=1)\n",
        "        input = torch.cat((input, idxs), dim=1)\n",
        "\n",
        "    out_str = ''.join([dataset.itos[int(i)] for i in input[0].tolist()])\n",
        "\n",
        "    return out_str"
      ],
      "metadata": {
        "id": "p1Lct4OaJHC7",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading saved model to use for inference\n",
        "model_save_name = 'shakespeareGPT.pt'\n",
        "path = f'/kaggle/working/{model_save_name}'\n",
        "lit_model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "1_bWPMB48n6J",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Who art thou?' # put your propmt here!\n",
        "preds_str = get_predictions(lit_model, prompt)\n",
        "print(preds_str)"
      ],
      "metadata": {
        "id": "IXzeF7Z3FyYY",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}